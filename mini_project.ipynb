{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class FiniteDifferenceOperator(nn.Module):\n",
    "    def __init__(self, Nleft, Nright, num_layers, hidden_dim):\n",
    "        super(FiniteDifferenceOperator, self).__init__()\n",
    "        # Number of total nodes in finite difference stencils\n",
    "        self.Nstencil = Nleft + Nright + 1\n",
    "        self.Nleft = Nleft\n",
    "        self.Nright = Nright\n",
    "\n",
    "        # The learnable part of the stencil\n",
    "        self.stencil = torch.nn.Parameter(torch.randn(self.Nstencil))\n",
    "        # The diffusion parameter. To enforce positivity, we evolve the log of the magnitude\n",
    "        self.logstabilizer = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))\n",
    "\n",
    "        # Construct nonlinear network\n",
    "        # -- The first layer scales up from Nstencil point to a hidden dimension (Input layer)\n",
    "        # -- Next, hidden layers repeatedly alternate between linear transforms and nonlinear activations (Hidden layers)\n",
    "        # -- Finally, a linear layer maps back down to Nstencil outputs\n",
    "        # We'll use skip connections in the hidden layer to improve numerical stability\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(self.Nstencil, hidden_dim, dtype=torch.float64))\n",
    "        layers.append(nn.Tanh())  # Activation function\n",
    "\n",
    "        # Hidden layers\n",
    "        # First make a list of hidden layers\n",
    "        hiddenlayers = []\n",
    "        for _ in range(num_layers):\n",
    "            block = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim, dtype=torch.float64), nn.Tanh()\n",
    "            )\n",
    "            hiddenlayers.append(block)\n",
    "\n",
    "        # Next apply skip connections\n",
    "        class SkipConnectionBlock(nn.Module):\n",
    "            def __init__(self, block):\n",
    "                super(SkipConnectionBlock, self).__init__()\n",
    "                self.block = block\n",
    "\n",
    "            def forward(self, x):\n",
    "                return x + self.block(x)\n",
    "\n",
    "        skip_connection_layers = [SkipConnectionBlock(block) for block in hiddenlayers]\n",
    "        hidden_layers = nn.Sequential(*skip_connection_layers)\n",
    "        layers.extend(hidden_layers)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(hidden_dim, self.Nstencil, dtype=torch.float64)\n",
    "        output_layer.weight.data.fill_(0.0)  # Set weights to zero\n",
    "        output_layer.bias.data.fill_(0.0)  # Set bias to zero (if any)\n",
    "        layers.append(output_layer)\n",
    "\n",
    "        self.nonlinear_network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Apply the finite difference stencil to the gridfunction x, assuming periodic BC\n",
    "        N_nodes = x.shape[0]\n",
    "        f_out = torch.zeros_like(x)\n",
    "        for i in range(N_nodes):\n",
    "            # Wrap indices periodically using the modulo operator (%)\n",
    "            indices = [\n",
    "                (i + j - self.Nleft) % (N_nodes - 1) for j in range(self.Nstencil)\n",
    "            ]\n",
    "\n",
    "            # Grab solution at indices\n",
    "            xstencil = x[indices]\n",
    "\n",
    "            # Hard coded diffusion operator\n",
    "            D2x = h ** (-2) * (\n",
    "                x[(i - 1) % (N_nodes - 1)] - 2 * x[i] + x[(i + 1) % (N_nodes - 1)]\n",
    "            )\n",
    "\n",
    "            # Apply neural network to xstencil\n",
    "            nonlinear_output = self.nonlinear_network(xstencil)\n",
    "\n",
    "            # Add constraint that sum of stencil coefficient = 0\n",
    "            nonlinear_output[-1] = -torch.sum(nonlinear_output[:-1])\n",
    "\n",
    "            # Apply learned stencil to xstencil, including the viscosity term\n",
    "            # f_out[i] = torch.sum(self.stencil * xstencil) + torch.exp(self.logstabilizer)*D2x\n",
    "            f_out[i] = (\n",
    "                torch.sum(nonlinear_output * xstencil)\n",
    "                + torch.exp(self.logstabilizer) * D2x\n",
    "            )\n",
    "\n",
    "        # Return stencil applied to current state consisting of nonlinearity and stabilizing diffusion\n",
    "        return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamiltonianNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # either use learned T (self.T) or true T (self.true_T)\n",
    "        # Kinetic energy network T(p)\n",
    "        self.Tnet = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "        # Potential energy network V(q)\n",
    "        self.Vnet = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def T(self, p):\n",
    "        \"\"\"Kinetic energy T(p)\"\"\"\n",
    "        return 0.5 * p.norm(dim=-1) ** 2\n",
    "        # return self.Tnet(p)\n",
    "\n",
    "    def V(self, q):\n",
    "        \"\"\"Potential energy V(q)\"\"\"\n",
    "        return self.Vnet(q)\n",
    "\n",
    "    def hamiltonian(self, q, p):\n",
    "        \"\"\"Compute Hamiltonian H = T(p) + V(q)\"\"\"\n",
    "        return self.T(p) + self.V(q)\n",
    "\n",
    "    def forward(self, q, p, dt):\n",
    "        \"\"\"Leapfrog integration step\"\"\"\n",
    "        # Set requires_grad=True for q and p so that we can take derivatives of the Hamiltonian\n",
    "        q = q.requires_grad_(True)\n",
    "        p = p.requires_grad_(True)\n",
    "\n",
    "        # Half step in momentum\n",
    "        dV = torch.autograd.grad(self.V(q).sum(), q, create_graph=True)[0]\n",
    "        p_half = p - 0.5 * dt * dV\n",
    "\n",
    "        # Full step in position\n",
    "        dT = torch.autograd.grad(self.T(p_half).sum(), p_half, create_graph=True)[0]\n",
    "        q_new = q + dt * dT\n",
    "\n",
    "        # Half step in momentum\n",
    "        dV = torch.autograd.grad(self.V(q_new).sum(), q_new, create_graph=True)[0]\n",
    "        p_new = p_half - 0.5 * dt * dV\n",
    "\n",
    "        return [q_new, p_new]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
